p 3
3: Stochastic processes and basic models for time series
Presentation
We will present the concept of time series model, through the relationship between time series and the theory of stochastic processes.

Next, we will cover the most basic models and their statistical properties, with an emphasis on stationarity conditions.

Goals
Examine simple time series models and their properties;
Investigate the stationarity of a model for time series;
Analyze candidate models to represent some types of series.
Stochastic Processes
A stochastic process is a set of random variables indexed in time or space. For our purpose in this course, we will cover time-indexed processes. In this way, we can define a stochastic process as a set or a sequence of random variables {Yt}t=1:T, or even: {Y1, Y2, ..., YT}.
It is also possible to define a continuous time stochastic process, in other words, in which the time parameter takes values â€‹â€‹in a continuous interval, but they are not as important for time series analysis purposes.
A random variable, by definition, can take values â€‹â€‹in a set, with each value (or range of values) having an associated probability, thus defining a probability distribution for the random variable.

See an example below.

Example 3.1 - We can have an X variable with the following probability distribution:

x P(X = x)
1 1/2
2 1/3
3 1/6
where the probability of X taking, say, the value 2 is equal to 1/3.

This set can be discrete (as in the case of example 3.1: {1, 2, 3}) or continuous (as the interval between 0 and 1 or even the set of all real numbers, as in the case of the famous distribution of Normal probability, which will even be quite useful for our purposes).

know more
If the random variables that define the stochastic process are discrete, we say that the process is discrete. If they are continuous, we say that the process is continuous. Both types of stochastic processes are important in time series theory, although, within the models developed in this course, continuous processes are more important.
Now, as a process is composed of random variables, and each one of them, by the definition above, can assume a set of values â€‹â€‹(possibly infinite), it is concluded that each stochastic process can generate a set, or sequence (since we are indexing time variables) of numerical values.

These values â€‹â€‹are called achievement or trajectory. The set of all possible realizations of a stochastic process is called an ensemble.
We finally arrive at the formal definition of a time series.

A time series is nothing more than a possible realization of a stochastic process, which we denote by {y1, y2, ..., yT}.

It should be clear that, in practice, what we observe is not the stochastic process but the series. Therefore, it is important to understand that the fact that a series is generated by a stochastic process is just an assumption.

Based on this assumption, our objective is to identify the process that generated the series, called the data generating process, or, as is more usual, the model that represents the series.

Modeling a time series is essentially discovering the statistical characteristics of its generating process. This means, in principle, that for each instant t we would need to discover the properties of the corresponding variable Yt. However, this task seems impossible, since we only have a single observation of this variable (yt).

The ergodicity property is then defined. A stochastic process is called ergodic if, from a single realization {y1, y2, ..., yT}, it is possible to discover the characteristics of the theoretical process: {Y1, Y2, ..., YT}.
Comment
Although the task seems impossible when thinking about each instant t individually, it may be possible when the observations are considered together.

Basically, what is done is to impose â€œsimilarityâ€ restrictions between the variables {Yt}t=1:T. These restrictions, which we will formally define later as stationarity, together with the assumption of ergodicity of the generating process (notice that this is an assumption), allow us to make statistical inferences about the model.

In other words, we need them so that it is feasible to carry out the task of modeling a time series.

In the context of time series, a stochastic process is usually represented by an equation that describes the evolution of ğ‘Œğ‘¡, for t = 1, 2, 3, ..., T, where T is the total of available observations. An example is shown below:

An example of a stochastic process is:

Yt=0.8Yt-1+Îµt.

As we will see later, this type of stochastic process is called autoregressive. The name is due to the fact that the series value at t is regressed by its own lagged value.

know more
The term ğœ€ğ‘¡ is the random error, or error term, which represents the fact that ğ‘Œğ‘¡ is not exactly equal to 0.8ğ‘Œ(ğ‘¡âˆ’1. In fact, if this were to happen, the series value at t would be possible to be predicted exactly from its value at the previous moment, which, let's face it, is not compatible with reality.
Also, if it were possible to accurately determine the value of ğ‘Œğ‘¡ from ğ‘Œğ‘¡âˆ’1, then you would be taking a course to learn to predict time series, would you agree?

A process accurately predicted from the value at t-1 would be called deterministic. The name stochastic refers precisely to the uncertainty inherent in the future value of ğ‘Œğ‘¡. A process, stochastic, therefore, necessarily involves the specification of an error term. This term has very specific properties, as we see below.

3.2 - White noise
The error term plays, in a time series model, a role similar to that of the error term in a regression model. It even has the same properties as that, as we will see below. It also appears in the decomposition models presented in chapter 1: it is about the irregular component (go back to class 1 and see that the notation matches).

In time series theory, Îµ_t is called white noise. The origin of this nomenclature is in engineering (noise in systems), and it is related to the fact that white is the resultant color of all the others, just as the error of a model is composed of all other factors that may influence the variable dependent, in the case Y_t.

White noise has the following properties:

EÎµt=0.âˆ€t
VÎµt=Ïƒ2,âˆ€t
CorrÎµi,Îµj=0,âˆ€iâ‰ j.

A particularly important case is Gaussian white noise, defined as follows:

Îµt~i.i.d.N0,Ïƒ2,âˆ€t.

where the acronym i.i.d. means: Independent and identically distributed.
It is Gaussian white noise, sometimes also called strictly white noise, that will be used to represent the error term in the time series models we will study in this course. These stochastic models or processes are called Gaussians.

 stationarity
 Click the button above.
3.3 - Stationarity
A stochastic process is said to be stationary (in the strong, strict or broad sense) if its characteristics do not change over time. In general, this condition is very difficult to verify, and it is common to adopt a simpler - or weaker - notion of stationarity.

A PE is said to be weakly stationary, weakly stationary, 2nd order stationary, or even covariance-stationary, if all three of the following conditions are satisfied:

EYt=Î¼,âˆ€t=1,2,...,T(constant mean)
VYt=Ïƒ2,âˆ€t=1,2,...,T(constant variance)
CovYt,Yt-k=Î³k(Cov is a function of k only!)âˆ€t=1,2,...,T

where E(Yt) and V(Yt) are, respectively, the expected value and the variance of Yt , and Cov(Yt,Yt-k) is the covariance of Yt and Yt-k.

Note that covariance is constant over time, but varies with k. This means that Cov(Y3,Y2) = Cov(Y2,Y1) (both denoted by Ï’1) but, for example, Cov(Y3,Y1) â‰ Cov(Y2,Y1).

As we will see later, under weak stationarity, ï§k is called Yt's autocovariance function (the name has the same justification as the case of an autoregressive model: It basically represents the covariance of the series with itself, lagged).

Note that white noise is, by definition, a stationary stochastic process.

The good news is that, if a stochastic process is Gaussian (that is, it has Gaussian white noise in its composition), it is enough to check the weak stationarity conditions, since they also imply strong stationarity.

In fact, the Normal distribution is completely characterized by expected value, variance and covariance, hence the fact that both definitions of stationarity coincide. All stochastic processes studied throughout this course will be Gaussian, so verifying the weak stationarity conditions stipulated above is sufficient.
The importance of stationarity is that, by imposing the restrictions that the parameters that define the stochastic process do not vary over time, the task of estimating them from a single realization, which consists of a set of T remarks.

Strictly, stationarity is only a necessary condition for the ergodicity of a process. However, for the models in this course, checking stationarity is enough.

It is noteworthy that non-stationary processes can also be used to represent time series (which have a trend, for example), but the series needs to undergo a â€œtreatmentâ€ before estimation. This subject will be taken up later in the course.
3.4 - Random Walk (simple)
The simplest and most important stochastic process is called random walk, for which the original English term is often used: Random walk (there are several common anglicisms in time series theory).

The simple random walk is defined as follows:

y_t=Y_(t-1)+Îµ_t.

Note the similarity with the equation in the stochastic process example, with the exception of coefficient 0.8. It should be noted that this simple modification significantly alters the properties of the stochastic process, with implications for the analysis of time series, as we will see later.

know more
The name random walk comes from the fact that, if a person is in a certain position at time t-1, he can be in any position at time t, this new position being defined by the value assumed by the random term Îµ_t.

For simplicity, one can think of ğœ€_ğ‘¡ as a random variable that takes three possible values: -1, 0 and 1. If ğœ€ğ‘¡= ğ‘™ğ‘œğ‘Ÿğ‘’ we would have one step to the left. If ğœ€ğ‘¡= 1, we would have one step to (front and to) right. If ğœ€ğ‘¡=0, the person walks forward in a straight line.

Note, however, that because the error term has a Normal distribution, the person's position after the next step is given by the value of a continuous random variable. An analogy is often made with the walk of a drunk.

Let's check if the "simple random walk is stationary?
Solution:

Y1=Y0+Îµ1(let's assume Y0=0)
Y2=Y1+Îµ2=Îµ1+Îµ2
Y3=Y2+Îµ3=Îµ1+Îµ2+Îµ3

The general formula for a generic t instant is:

Yt=âˆ‘Îµi,i=1t

Whose expected value is:

EYt=Eâˆ‘i=1tÎµi=âˆ‘i=1tEÎµi=0,

since ğ¸(ğœ€ğ‘–)=0, by definition.

We see, therefore, that the first stationarity condition was satisfied (the expected value does not depend on t). We say that this process is stationary on average. Now let's calculate the variance to check the second condition. Remembering that ğœ€ğ‘–`s are uncorrelated, by definition, we have that the variance of ğ‘Œğ‘¡ is given by:

VYt=VtÏ†0+âˆ‘ i=1 tÎµi

=VtÏ†0+Vâˆ‘Îµii=1t

=Vâˆ‘Îµii=1t=âˆ‘Vi=1tÎµi=tÏƒ2,

because ğ‘‰(ğœ€ğ‘– )=ğœ2 by definition.

Heads up
The variance depends on time. Thus, we conclude that the simple random walk is non-stationary, as the second condition was violated.

The following image presents the graph of a random walk without constant.
It is possible to notice that, although there is a strong oscillation of the process, in the long run it tends to fluctuate around zero, confirming the behavior dictated by its expected value. It is important to emphasize that the concept of expected value refers to the average of the values â€‹â€‹of the series in the long term, that is, over an infinite horizon.

It does not mean that, if we add the values â€‹â€‹of the series over a finite period, the result is exactly zero (although it should be close to this value, if the period considered is not very small).

3.5 - Random walk with constant
Random walk plus drift is defined as follows:

Yt=Ï•+Yt-1+Îµt.

Although the role of the constant seems similar to that of the intercept in a linear regression model, the presence of the constant ï¦ in a time series model is much more striking. In the case of the random walk, in particular, it significantly alters the characteristics of the process.

Now, let's check if the random walk with constant is stationary.

Solution:

Y1=Ï•+Îµ1(assuming Y0=0)
Y2=Ï•+Y1+Îµ2
=Ï•+Ï•+Îµ1+Îµ2
=2Ï•+Îµ1+Îµ2
Y3=Ï•+Y2+Îµ3
=3Ï•+Îµ1+Îµ2+Îµ3

The general formula for a generic t instant is:

Yt=tÏ•+âˆ‘Îµii=1t,

whose expected value is:

Yt=tÏ•+Eâˆ‘i=1tÎµi=

tÏ•+âˆ‘Ei=1tÎµi=tÏ•.
Thus, we prove that the random walk with constant is non-stationary, since the first stationarity condition was violated (the expected value depends on t). However, let's also calculate the variance. Again, using that ğœ€ğ‘–`s are uncorrelated, by definition, we have that the variance of ğ‘Œğ‘¡ is given by:

VYt=VtÏ•+âˆ‘Îµii=1t

=VtÏ•+Vâˆ‘Îµii=1t

=Vâˆ‘Îµii=1t=âˆ‘Vi=1tÎµi=tÏƒ2.

Equation ğ¸(ğ‘Œğ‘¡) ğ‘‰(ğ‘Œğ‘¡)
ğ‘Œğ‘¡=ğ‘Œğ‘¡âˆ’1+ğœ€ğ‘¡ 0 ğ‘¡ğœ2
ğ‘Œğ‘¡=Ï† + Y(tâˆ’1)+ğœ€ğ‘¡ tÏ† ğ‘¡ğœ2
Table 3.1 - Random Walk Properties (with and without added constant). Source: Author

The figure below shows the graph of a random walk with a constant (negative).
It is possible to observe that, as expected, the presence of a negative constant causes the trajectory of the series to present a downward trend in the long run.

An important point to be highlighted again is the importance of the added constant, in this case, Ï†, in a time series model.

In the random walk example, we saw that it has the power to change a property of the stochastic process (making it non-stationary on average). This logic will also apply to the more complex time series models that we will study throughout this course.

3.6 - Model AR(1)
Returning to the model in example 3.1, but introducing a general constant Ï† (which in the example cited assumed the value 0.8), we have:
Yt=Yt-1+Îµt,

know more
We have already seen that this model is called autoregressive. It is also said to be of order 1, that is, it is an autoregressive model of order 1, or, as it is common to denote it: AR(1).

It is possible to expand this model to consider more lags of ğ‘Œğ‘¡, that is, to expand its order. The constant ğœ‘ is a process parameter. Later, we will see how to estimate this and other time series model parameters. For now, let's focus on checking the stationarity of this model.

An AR(1) model is stationary if the coefficient Ï† is, in magnitude, less than 1. See a demonstration below.

Let's follow the same line as what was done with the random walk.

At t=1, we have:

Y1=Îµ1(assuming Y0=0)
Y2=Ï•Y1+Îµ2=Ï•Îµ1+Îµ2
Y3=Ï•Y2+Îµ3=Ï•2Îµ1+Ï•Îµ2+Îµ3
Y4=Ï•Y3+Îµ4=Ï•3Îµ1+Ï•2Îµ2+Ï•Îµ3+Îµ4

The general formula for a generic t instant is:

Yt=âˆ‘Ï•ii=0t-1Îµt-1,

Whose expected value is:

EYt=Eâˆ‘i=0t-1Ï•iÎµt-1=âˆ‘i=0t-1Ï•iEÎµt-1=0,

since ğ¸(ğœ€ğ‘–)=0, by definition.

The variance of ğ‘Œğ‘¡ is (the ğœ€ğ‘–`s are uncorrelated):

VYt=Vâˆ‘Ï•ii=0t-1Îµt-1

=âˆ‘i=0t-1Ï•2iVÎµt-1

âˆ‘i=0t-1Ï•2iÏƒ2=Ïƒ2âˆ‘i=0t-1Ï•2i.

Now notice that the terms inside the summation define a geometric progression: Ï†0, Ï†2,Ï†3,Ï†4, ..., up to Ï†2(ğ‘¡âˆ’1), that is, with first term 1 (since Ï†0 = 1) and ratio Ï†2 ).

Remembering now that the sum of the terms of a finite geometric progression with t terms, the first term being equal to a1 and the ratio equal to q, is given by:

St=a11-qt1-q

And noting that, in the case in question, a1 = 1 and q=Ï•2, we have:

âˆ‘Ï•2ii=0t-1=1-Ï•2t1-Ï•2
And so:

VYt=Ïƒ2âˆ‘Ï•2ii=0t-1=1-Ï•2t1-Ï•2Ïƒ2.

Thus, we have that the variance depends on t. Note however that if t tends to infinity (t is considered infinite for stationarity analysis), the above result is constant since Ï•<1, in which case the term Ï•2t approaches zero. In this case:

VYt=Ïƒ21-Ï•2,

that does not depend on t. Thus, the AR(1) model is stationary if and only if Ï•<1, confirming that this is the stationarity condition of this model.

Like the random walk, the AR(1) model can also contain an added constant. In this case, this constant is denoted by Ï•0 and the constant Ï• in the previous example is denoted by Ï•1. We have it like this:

Yt=Ï•0+Ï•1Yt-1+Îµt,

We now have two parameters in the model, which we will later learn to estimate.

As an example, let's check whether the constant Ï•0 affects the stationarity conditions of the AR(1) model and, if so, how.

Reproducing the development made for the model without constant, we arrived at:

Yt=Ï•0âˆ‘Ï•1ii=0t-1+âˆ‘Ï•1ii=0t-1Îµt-1,

The difference in relation to the previous case is only the first term, which is a constant and not a random variable. In this way, its variance is zero, such that the expression of the variance remains the same. Its expected value, however, is equal to itself Ï•0âˆ‘Ï•1ii=0t-1 and like this:

EYt=Ï•0âˆ‘Ï•1ii=0t-1

As for the sum, we can apply the formula for the sum of the geometric progression:

âˆ‘Ï•1ii=0t-1=1-Ï•1t1-Ï•1.

Applying the obtained expression to the expected value:

EYt=Ï•0âˆ‘Ï•1ii=0t-1=Ï•01-Ï•1t1-Ï•1.

Again, if Ï•1<1, Ï•1t tends to zero when t tends to infinity, and in this case:

EYt=Ï•01-Ï•1,

that does not depend on t. We thus have that the stationarity condition of the AR(1) model remains the same when we introduce the constant Ï•0, that is: Ï•1<1
It should be noted that, in a stationary AR(1) model, the expected value of the model is no longer equal to zero, but rather depends on the model parameters. Thus, when specifying an AR(1) model for a series whose mean over time is different from zero (which corresponds to the vast majority of cases), Ï•0 must be present.

In summary, the properties of the AR(1) model are summarized in the following table.


î’ Table 3.2 - AR(1) Model Properties (with and without added constant). 
See the graph of an AR(1) model with parameters Ï•0 = 4.5 and Ï• = 0.1.
Note that, in this case, the expected value of the model is given by:

EYt=Ï•01-Ï•1=4.51-0.1=4.50.9=5.

In fact, it is possible to notice that the trajectory of the generated series oscillates around the value 5.

The stationarity conditions for more complex models involve the concept of lag operator and characteristic equation, and will be explored in class 4, when more complex models and other properties will also be presented.

Activities
1. Which of the following characteristics of a stochastic process, supposedly generator of a time series, is incompatible with the stationarity property?

a) ğ¸(ğ‘Œğ‘¡ )â‰  0
b) ğ‘‰(ğ‘Œ_ğ‘¡ )=500
c) Cov(ğ‘Œğ‘¡,ğ‘Œ(ğ‘¡1))â‰  Cov(ğ‘Œğ‘¡,ğ‘Œğ‘¡âˆ’2))
d) Cov(ğ‘Œğ‘¡,ğ‘Œ(ğ‘¡âˆ’1))â‰  Cov(ğ‘Œ(ğ‘¡âˆ’1,ğ‘Œğ‘¡âˆ’2)
e) Cov(ğ‘Œğ‘¡âˆ’1,ğ‘Œğ‘¡âˆ’3) = Cov ğ‘Œ(ğ‘¡âˆ’2,ğ‘Œğ‘¡âˆ’3)

D. If the process is stationary, then the covariance of lag 1 is: ğ‘ªğ’ğ’—(ğ’€ğ’•,ğ’€ğ’•âˆ’ğŸ) = ğ‘ªğ’ğ’—(ğ’€ğ’•âˆ’ğŸ,ğ’€ğ’•âˆ’ğŸ)= ğ‘ªğ’ğ’—(ğ’€ğ’•âˆ’ğŸ,)ğ’€ğ’•âˆ’ğŸ‘) =â€¦
The expected value of the model ğ‘Œğ‘¡=2 + ğ‘Œğ‘¡âˆ’1)+ğœ€ğ‘¡, where Îµt~i.i.d. N (0.1), âˆ€ğ‘¡, is:

a) 0
b) 1
c) 2
d) 2t
e) 2 + ğ‘Œğ‘¡âˆ’1


Correct answer: letter D. The expected value of the random walk with constant is:
EYt=tÏ•+Eâˆ‘Îµii=1t=

tÏ•+âˆ‘Ei=1tÎµi=tÏ•=2t.

3. The following is the annual series of bituminous coal production in the United States between 1920 and 1968.
Which of the following models would be a candidate to represent this series?

a) simple random walk
b) random walk with constant
c) AR(1) stationary without constant
d) AR(1) stationary with constant
e) Lorem ipsum dolor sit amet, consectetur adipiscing elit.
Congratulations! You're right!

Correct answer: letter D. The process is stationary, which rules out B, and with a non-zero mean, which rules out A and C. D is a possible candidate, the only one among the proposed alternatives.
